\section{Discussion}

The results highlight a major challenge in the preprocessing pipeline: the
correct formatting of input documents. When using \texttt{python-docx}, footers
are not reliably removed, and paragraph boundaries are inconsistently
extracted. This significantly impacts model performance—inputs derived directly
from DOCX files led to noticeably worse results compared to manually formatted
markdown documents. These findings underscore the critical importance of
structured, well-formed input when working with language models in clinical
document analysis.\cite{p1}
Further experiments revealed that even minor variations in the system prompt—such as the presence or absence of a comma or blank line—can alter the model's output. This sensitivity suggests that prompt design and input consistency must be treated as part of the optimization process.

Another complicating factor is the lack of a universally accepted definition of what constitutes a "complete" medical discharge report. Standards may vary not only across countries but even between institutions, making generalization difficult.

In this work, the agentic evaluation setup focused exclusively on the diagnosis section, as this was not reliably detected as incomplete in the baseline setup—even when using a properly formatted document. In principle, the same methodology could be applied to other sections, though this would further increase computational cost and latency.

One potential direction for future work is the integration of a retrieval-augmented generation (RAG) component. This would allow individual clinics to provide their own reporting conventions and medical standards, enabling more context-aware evaluations. Combined with fine-tuning on high-quality institution-specific data, this approach could not only improve section completeness detection but also allow the model to adapt its tone and terminology to local clinical norms.

