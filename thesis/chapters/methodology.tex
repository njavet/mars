\section{Methodology}

To evaluate the capabilities of various language models (LLMs) in 
processing medical discharge reports, a multi-stage evaluation pipeline 
was implemented. The process consisted of document preprocessing, 
prompt engineering, model inference, and result evaluation using defined metrics.

Initially, raw discharge summaries in DOCX format were parsed 
using the \texttt{python-docx} library. This stage focused on extracting 
plain text content from the original documents while preserving section 
headers and structural elements. The extracted text was then sent to nine 
different LLMs using a uniform system prompt designed to assess completeness and 
relevance of medical sections.

Subsequently, to reduce variability introduced by inconsistent formatting, 
the documents were manually reformatted to adhere to a markdown-like structure. 
These manually structured texts were then re-evaluated using the same baseline 
system prompt to measure improvements attributable solely to input clarity.

In the final stage, the same markdown-formatted texts were reprocessed using 
a refined system prompt with more explicit instructions aimed at improving model 
focus and output consistency. This allowed for a comparative assessment of 
prompt sensitivity across models.

The evaluation metrics comprised both structural and content-based indicators: 
true positives, true negatives, false positives, and false negatives regarding 
section detection; as well as binary assessments of section relevance and conciseness 
(\texttt{0/1} scale). Sections marked as missing or irrelevant were explicitly 
recorded to aid in qualitative analysis.

The evaluation pipeline consisted of the following stages:

    Data Preparation
    A corpus of psychiatric discharge reports was converted from .docx to markdown using python-docx, with manual post-processing to standardize section headers and content layout.

    Baseline Evaluation
    Each markdown-formatted document was evaluated using nine different LLMs under a shared system prompt. The models' outputs were compared against a human-annotated reference.

    Prompt and Parameter Tuning
    Experiments with modified system prompts and various temperature settings were conducted to assess their impact on performance.

    Agentic Workflow
    A second phase introduced tool use and inter-model collaboration. One model (planner) orchestrated evaluation plans, delegating judgment and justification tasks to others. This setup aimed to simulate a form of self-verifying reasoning.

    Evaluation Metrics
    We measured:

        True/False Positives and Negatives (per section)

        Irrelevant response ratio (0/1)

        Conciseness of justification (0/1)
