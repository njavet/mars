\section{Introduction}
The ability of language models to analyze medical documents is of increasing importance in clinical decision support and medical auditing. While commercial models like ChatGPT have demonstrated strong performance in natural language understanding tasks, they pose privacy and latency concerns in sensitive domains. This work investigates whether small, open-source LLMs running on local hardware can provide a viable alternative for evaluating medical discharge reports. The task centers on identifying missing or incomplete sections in structured documents â€” a challenge requiring nuanced understanding of clinical content. We explore the potential of formatting, prompt engineering, and agentic workflows to enhance local model performance.
