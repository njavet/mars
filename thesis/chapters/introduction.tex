\section{Introduction}
Large language models (LLMs) have rapidly advanced natural language understanding across domains such as education, law, and medicine.\cite{p0} In clinical settings, they offer new opportunities for automating tasks like summarization and document quality control. Medical discharge reports, in particular, require careful verification of structure and content completeness—tasks that demand not only linguistic fluency but also medical reasoning.

State-of-the-art proprietary systems such as ChatGPT demonstrate impressive
performance in these tasks but come with significant drawbacks: high
computational cost, opaque training processes, and regulatory concerns
regarding data privacy. These limitations hinder their adoption in sensitive
environments like healthcare. Moreover it is important to distinguish that
chatGPT is not merely a language model but a full-stack application that
integrates one or more LLMs (e.g. GPT-4) with additional infrastructure, memory
components, and reinforcement-based interaction layers.

Open-source LLMs in the 7–8 billion parameter range provide an attractive alternative due to their local deployability and transparency. However, their capacity to handle complex medical evaluation tasks remains insufficiently validated. This thesis investigates whether such models, when carefully adapted through prompt engineering and agentic workflows, can approach the performance of their proprietary counterparts in the domain of clinical discharge report analysis.

\vspace{2\baselineskip}

\textbf{This thesis addresses the following research question:}

\begin{quote}
Can small, open-source language models (~8B parameters) be optimized to evaluate the completeness of medical discharge reports at a level comparable to that of proprietary models such as ChatGPT?
\end{quote}

To answer this question we focus on the task of section-level completeness analysis: 
identifying whether critical information (e.g., diagnoses, medication, follow-up plans) 
is present and appropriately described. 
To this end, we develop an evaluation framework that incorporates reference annotations, 
model-guided assessments, and performance metrics such as precision, recall, and relevance.
We further explore prompt engineering techniques, instruction design, 
and lightweight agentic workflows to enhance model behavior without fine-tuning. 

\clearpage

