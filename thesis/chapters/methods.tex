\section{Methods}

To evaluate the capabilities of various language models (LLMs) in 
processing medical discharge reports, a multi-stage evaluation pipeline 
was implemented. The process consisted of document preprocessing, 
prompt engineering, model inference, and result evaluation using defined metrics.
The dataset consists of seven discharge reports

\subsection{Dataset}
The dataset used for evaluation consists of seven medical discharge reports. 
Among them, two reports are considered complete: one includes section-level 
annotations and one does not. 
\begin{table}[H]
\centering
\begin{tabular}{l l}
\toprule
\textbf{Report} & \textbf{Missing Sections} \\
\midrule
Report 1 & Medical History \\
Report 2 & Substance use history \\
Report 3 & Psychopharmacology \\
Report 4 & Clinical course \\
Report 5 & Diagnoses \\
\bottomrule
\end{tabular}
\caption{Overview of missing sections in the five incomplete discharge reports.}
\label{tab:missing_sections}
\end{table}
Report 1 and 2 do have empty sections, while Report 3 and 4 do have valid
german statements, but without any information. Report 5 does have a valid
diagnosis, but is not formal enough.

\subsection{Evaluation Metric}
To assess the performance of the language models in identifying incomplete sections within medical discharge reports, we define the following evaluation categories:
\begin{itemize}
  \item \textbf{True Positive (TP)}: The model correctly identifies a section as incomplete when it is indeed missing or insufficient according to the reference annotation.
  \item \textbf{False Negative (FN)}: The model incorrectly classifies an incomplete section as complete, thereby failing to detect the omission.
  \item \textbf{Irrelevant (IR)}: The model output diverges from the prompt instructions, such as by hallucinating content, introducing unrelated information, or failing to perform the evaluation task.
\end{itemize}

\subsection{Language Model Configuration}
For all model evaluations, a deterministic generation configuration was used 
to promote consistency and minimize variance across runs. 
Specifically, we selected a low temperature (\texttt{temperature = 0.0}) 
along with constrained sampling parameters (\texttt{top\_p = 0.4},
\texttt{top\_k = 8}). 
These settings reduce randomness in token selection and encourage the 
model to produce concise, focused outputs aligned with the most probable completions.
The rationale behind this choice is twofold. 
First, clinical report evaluation is a task that prioritizes factual precision 
and structural adherence over creativity or linguistic diversity. 
Second, high-temperature outputs tend to increase hallucination rates and irrelevance, 
which directly impacts the reliability of model judgments in a safety-critical 
domain such as medicine.
By enforcing a narrower decoding distribution, we ensure that the model’s responses 
remain stable and reproducible, which is essential for systematic error 
analysis and metric comparison across different prompt configurations.
Each model was invoked with a system message tailored to the specific evaluation task. 
For instance, when assessing section completeness, 
the system message defined what constitutes a complete diagnosis section, 
how to handle missing or ambiguous information, and what format the response 
should follow (e.g., JSON structure indicating completeness).

By default, Ollama enforces a maximum context length of 2024 or 4096 tokens, 
which proved insufficient for full-length medical discharge reports. 
As a result, a custom \texttt{Modelfile} was created for each model to 
explicitly raise the \texttt{num\_ctx} parameter. 
This allowed the models to process entire documents without truncation, 
ensuring accurate and contextually informed evaluations.

\subsection{Evaluation}
The following language models were evaluated in this study:

\begin{table}[H]
\centering
\begin{tabular}{l l}
\toprule
\textbf{Model} & \textbf{Parameter Size} \\
\midrule
Dolphin-LLaMA 3.1 & 8B \\
LLaMA 3.1 & 8B \\
LLaMA 3.1 Instruct (Quantized, q8\_0) & 8B \\
Mistral Instruct & 7B \\
Hermes 3 (LLaMA 3.1 base) & 8B \\
\bottomrule
\end{tabular}
\caption{Evaluated local language models and their parameter sizes.}
\label{tab:llm_models}
\end{table}

Initially, raw discharge summaries in DOCX format were parsed 
using the \texttt{python-docx} library. This stage focused on extracting 
plain text content from the original documents while preserving section 
headers and structural elements. The extracted text was then sent to the LLMs
using the following system prompt:
\begin{verbatim}
Du bist ein Evaluator für medizinische Austrittsberichte einer Psychiatrie.
Prüfe den Austrittsbericht auf fehlende medizinische Informationen und gib
**ein reines JSON-Objekt** nach folgendem Schema aus:

* `"Abschnittsname": 
0` wenn es Evidenz gibt, dass der Abschnitt medizinisch vollständig ist
* `"Abschnittsname": 
1` wenn nicht

Gib das JSON-Objekt OHNE Kommentare, Fließtext oder Einleitung zurück.
\end{verbatim}

Subsequently, to reduce variability introduced by inconsistent formatting, 
the documents were manually reformatted to adhere to a markdown-like structure. 
These manually structured texts were then re-evaluated using the same baseline 
system prompt to measure improvements attributable solely to input clarity.

In the final stage, the same markdown-formatted texts were reprocessed using 
an agentic workflow with access to a tool. 

