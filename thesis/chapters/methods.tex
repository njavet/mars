\section{Methods}

To evaluate the capabilities of various language models (LLMs) in 
processing medical discharge reports, a multi-stage evaluation pipeline 
was implemented. The process consisted of document preprocessing, 
prompt engineering, model inference, and result evaluation using defined metrics.

\subsection{Dataset}
The dataset used for evaluation consists of seven medical discharge reports. 
Among them, two reports are considered complete: one includes section-level 
annotations and one does not. 
\begin{table}[H]
\centering
\begin{tabular}{l l}
\toprule
\textbf{Report} & \textbf{Missing Sections} \\
\midrule
Report 1 & Medical History \\
Report 2 & Substance use history \\
Report 3 & Psychopharmacology \\
Report 4 & Clinical course \\
Report 5 & Diagnoses \\
\bottomrule
\end{tabular}
\caption{Overview of missing sections in the five incomplete discharge reports.}
\label{tab:missing_sections}
\end{table}
Report 1 and 2 contain section headers without meaningful content, while Report 3 and 4 do have valid
german statements, but without any information. Report 5 does have a valid
diagnosis, but lacks the structured format or specificity typically required
for clinical documentation.

\subsection{Evaluation Metric}
To assess the performance of the language models in identifying incomplete sections within medical discharge reports, we define the following evaluation categories:
\begin{itemize}
  \item \textbf{True Positive (TP)}: The model correctly identifies a section as incomplete when it is indeed missing or insufficient according to the reference annotation.
  \item \textbf{False Negative (FN)}: The model incorrectly classifies an incomplete section as complete, thereby failing to detect the omission.
  \item \textbf{Irrelevant (IR)}: The model output diverges from the prompt instructions, such as by hallucinating content, introducing unrelated information, or failing to perform the evaluation task.
\end{itemize}

\clearpage
\subsection{Language Model Configuration}
For all model evaluations, a deterministic generation configuration was used 
to promote consistency and minimize variance across runs. 
Specifically, we selected a low temperature along with constrained sampling
parameters.
\begin{table}[H]
\centering
\label{tab:llm_settings}
\begin{tabular}{c c c c}
\toprule
    \textbf{Context Window (CTX)} & \textbf{Temperature} & \textbf{top\_k} &
    \textbf{top\_p} \\
\midrule
    32768 & 0.0 & 8 & 0.4 \\
\bottomrule
\end{tabular}
\caption{LLM Settings}
\end{table}
These settings reduce randomness in token selection and encourage the 
model to produce concise, focused outputs aligned with the most probable completions.
The rationale behind this choice is twofold. 
First, clinical report evaluation is a task that prioritizes factual precision 
and structural adherence over creativity or linguistic diversity. 
Second, high-temperature outputs tend to increase hallucination rates and irrelevance, 
which directly impacts the reliability of model judgments in a safety-critical 
domain such as medicine.
By enforcing a narrower decoding distribution, we ensure that the model’s responses 
remain stable and reproducible, which is essential for systematic error 
analysis and metric comparison across different prompt configurations.
Each model was invoked with a system message tailored to the specific evaluation task. 
For instance, when assessing section completeness, 
the system message defined what constitutes a complete diagnosis section, 
how to handle missing or ambiguous information, and what format the response 
should follow (e.g., JSON structure indicating completeness).

\subsection{Context Length}
The context length (\texttt{num\_ctx}) of a language model defines how many tokens it can process in a single prompt. This parameter is critical for tasks involving long documents, such as medical discharge reports. Models like LLaMA 3.1 and Mistral are typically pretrained with maximum context lengths of 8192 or 32768 tokens. However, many runtime environments (e.g., \texttt{Ollama}) set lower default values for performance and stability reasons.

In this study, initial evaluations were conducted with the default setting of
\texttt{num\_ctx} = 2024, as configured by \texttt{Ollama}'s default Modelfile.
Since this was insufficient to fully encode several documents, a custom
\texttt{Modelfile} was created to raise the limit to 32768 tokens. The
effective context length depends not only on the model architecture but also on
implementation and available GPU memory. All models except
\texttt{llama3.1:8b-instruct-q8\_0} were executed using 4-bit
quantization on an RTX A2000 GPU (8GB VRAM).

\clearpage
\subsection{Software Architecture}
To support flexible experimentation with different prompting configurations, model parameters, and input formats, a minimal web-based application was developed. The system consists of a Python backend built with \texttt{FastAPI} and a lightweight frontend implemented using \texttt{Vue.js}. Communication between the frontend and backend is handled via asynchronous HTTP requests, enabling low-latency interactions with local language models served through the Ollama interface.

The backend is responsible for handling document uploads, formatting inputs (e.g., converting DOCX to plain text or markdown), selecting models, applying system prompts, invoking the language model, and collecting outputs. The frontend provides a user-friendly interface that allows users to select evaluation modes, inspect model responses, and track performance metrics in real time.

This architecture facilitates experimentation not only for technical users but also for non-technical stakeholders, such as clinicians or researchers, who may want to test how different models or prompts behave on actual medical documents. By abstracting away the complexity of model invocation and formatting logic, the system enables interactive exploration of language model behavior in a controlled and reproducible environment.

\subsection{Evaluation}
The following language models were evaluated in this study with observed VRAM
and RAM usage based on \texttt{nvidia-smi} and Linux \texttt{htop}

\begin{table}[H]
\centering
\begin{tabular}{l c c c}
\toprule
    \textbf{Model} & \textbf{Parameter Size} & \textbf{VRAM (GB)} & \textbf{RAM
    (GB)} \\ 
\midrule
    Command-r7b & 7B & 6.5-7.5 & ~4 \\
    Hermes 3 (LLaMA 3.1 base) & 8B & 6.5-7.5 & ~4  \\
    LLaMA 3.1 & 8B & 6.5-7.5 & ~4  \\
    LLaMA 3.1 Instruct (Quantized, q8\_0) & 8B & 6.5-7.5 & ~4  \\
    Mistral Instruct & 7B & 6.5-7.5 & ~4  \\
\bottomrule
\end{tabular}
\caption{Evaluated local language models and their parameter sizes.}
\label{tab:llm_models}
\end{table}
\clearpage

Initially, raw discharge summaries in DOCX format were parsed 
using the \texttt{python-docx} library. This stage focused on extracting 
plain text content from the original documents while preserving section 
headers and structural elements. The extracted text was then sent to the LLMs
using the following system prompt:
\begin{promptbox}
\begin{verbatim}
Du bist ein Evaluator für medizinische Austrittsberichte einer 
Psychiatrie. Prüfe den Austrittsbericht auf fehlende medizinische 
Informationen und gib **ein reines JSON-Objekt** nach 
folgendem Schema aus:

* `"Abschnittsname": 
0` wenn es Evidenz gibt, dass der Abschnitt medizinisch vollständig ist
* `"Abschnittsname": 
1` wenn nicht

Gib das JSON-Objekt OHNE Kommentare, Fließtext oder Einleitung zurück.
\end{verbatim}
\end{promptbox}

Subsequently, to reduce variability introduced by inconsistent formatting, 
the documents were manually reformatted to adhere to a markdown-like structure. 
These manually structured texts were then re-evaluated using the same baseline 
system prompt to measure improvements attributable solely to input clarity.

In the final stage, the same markdown-formatted texts were reprocessed using 
an agentic workflow with access to a tool. The system prompt for the agentic
workflow was modified to include the tool use:
\begin{promptbox}
\begin{verbatim}
Du bist ein Evaluator für medizinische Austrittsberichte einer 
Psychiatrie. Prüfe den Austrittsbericht auf fehlende medizinische 
Informationen und gib **ein reines JSON-Objekt** nach 
folgendem Schema aus:

* `"Abschnittsname": 
0` wenn es Evidenz gibt, dass der Abschnitt medizinisch vollständig ist
* `"Abschnittsname": 
1` wenn nicht

Beim Abschnitt 'Diagnosen' kannst du das Tool
`analyze_diagnosis(text: str)` verwenden.
Nutze es nur dann, wenn du dir unsicher bist, ob der 
Abschnitt vollständig ist.

Gib das JSON-Objekt OHNE Kommentare, Fließtext oder Einleitung zurück.
\end{verbatim}
\end{promptbox}

\clearpage
If the model decides to use the tool, the Agent then calls another LLM with the
following system prompt:
\begin{promptbox}
\begin{verbatim}
Du bist ein Spezialist für psychiatrische Diagnosen.

Beurteile, ob ein Diagnosen-Abschnitt vollständig ist.

Ein Abschnitt ist **vollständig**, wenn:
* Mindestens ein ICD-10-Code wie "F01" oder "F10.1" enthalten ist.
* Die Diagnosebeschreibung ist medizinisch konsistent mit dem ICD-Code.

**Diagnosebezeichnungen ohne ICD-10-Codes gelten als unvollständig.**

Antwortformat:

Wenn vollständig:
{"Diagnosen": 0}

Wenn unvollständig:
{"Diagnosen": 1}

Antwort ausschließlich im JSON-Format. Keine Kommentare. Kein Fließtext.
\end{verbatim}
\end{promptbox}


