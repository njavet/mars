\begin{abstract}
Large language models (LLMs) have shown strong performance in clinical text analysis but often rely on proprietary infrastructure with high resource demands. This thesis explores whether small, open-source LLMs (7â€“8 billion parameters) can be optimized to approximate the analytical capabilities of models like GPT-4, specifically in evaluating medical discharge summaries. A task-specific framework is developed to measure completeness, clinical relevance, and reasoning quality. We apply prompt engineering, system prompt refinement, and tool-augmented workflows to improve performance. Results show that, while smaller models have limitations in generalization and reasoning, targeted interventions significantly improve their utility, supporting the viability of resource-efficient deployment in clinical contexts.
\end{abstract}
\clearpage
