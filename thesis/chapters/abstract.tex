\begin{abstract}
This thesis investigates whether small, locally running large language models (LLMs) can achieve comparable performance to commercial cloud-based systems like ChatGPT in evaluating medical discharge reports. The evaluation focuses on identifying incomplete or missing information across structured markdown-formatted sections of clinical documents. Several strategies were explored, including optimized prompt engineering, manual formatting of Word documents, parameter tuning (e.g., temperature settings), and agentic workflows involving tool use and multi-model collaboration. Performance was assessed using precision, recall, and qualitative criteria such as conciseness and relevance. Results show that while small LLMs improve with structured input and specialized prompting, they remain limited in reliability compared to state-of-the-art proprietary models.
\end{abstract}
