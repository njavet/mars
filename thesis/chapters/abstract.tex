\begin{abstract}
Recent advances in large language models (LLMs) have demonstrated strong performance in medical text analysis tasks, including the evaluation of clinical discharge reports. However, the high computational requirements and proprietary nature of models such as ChatGPT limit their accessibility and transparency. This thesis investigates whether small, open-source LLMs—specifically models in the 7–8 billion parameter range—can be optimized to approach the analytical quality of proprietary systems in the context of medical discharge summary evaluation. We design a task-specific evaluation framework to assess model performance across dimensions such as section completeness, clinical relevance, and reasoning fidelity. Techniques such as prompt engineering, system message adaptation, and agentic workflows are applied to enhance the output of local models. Comparative analysis with GPT-4-based baselines reveals that, while small models exhibit limitations in reasoning depth and robustness, targeted optimization significantly narrows the performance gap. Our results highlight the viability of deploying resource-efficient LLMs for structured medical report analysis under constrained environments.
\end{abstract}
